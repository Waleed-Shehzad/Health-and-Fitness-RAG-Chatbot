# -*- coding: utf-8 -*-
"""Query RAG CHATBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10LY2Iu9QFLFf3DHiiaaqTfdSOTxtfeUx
"""

import textwrap
import torch
!pip install pinecone
import pinecone
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Initialize Pinecone
pc = pinecone.Pinecone(api_key="pcsk_g4i5v_Ds3ruCETwf5B516KCejmF2upXigJeLjPBhXWSUNq7f26AB33DFmfvoQ6ryaRyey")
index = pc.Index("roberta")

# Load embedding model
embedding_model = SentenceTransformer('FacebookAI/xlm-roberta-base')

def retrieve_context(query, top_k=10, min_score=0.75):
    query_vector = embedding_model.encode(["query: " + query])[0].tolist()
    response = index.query(vector=query_vector, top_k=top_k, include_metadata=True)

    return [
        match['metadata']['text']
        for match in response.get('matches', [])
        if match['score'] >= min_score
    ]

model_name = "teknium/OpenHermes-2.5-Mistral-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id)

def generate_answer(query, context_chunks, max_context_words=400):
    context = ""
    word_count = 0
    for chunk in context_chunks:
        word_count += len(chunk.split())
        if word_count > max_context_words:
            break
        context += chunk + "\n\n"

    prompt = (
        "You are a helpful multilingual assistant. "
        "Use the context below to answer the question accurately. "
        "Respond in the same language as the question.\n\n"
        f"Context:\n{context}\n"
        f"Question: {query}\n\n"
        "Answer:"
    )

    result = generator(prompt, max_new_tokens=200, do_sample=False)
    return result[0]['generated_text']

# Example query
query = "What are the five training variables described in the fitness handbook"
context_docs = retrieve_context(query)

if context_docs:
    answer = generate_answer(query, context_docs)
    final_answer = textwrap.fill(answer.split("Answer:")[-1].strip(), width=100)
    print("Question:", query)
    print("Answer:\n", final_answer)
else:
    print("No relevant context found.")

# Example query
query = "Quel changement dans l'alimentation l'auteur a-t-il fait qui a amélioré sa santé intestinale ?"
context_docs = retrieve_context(query)

if context_docs:
    answer = generate_answer(query, context_docs)
    final_answer = textwrap.fill(answer.split("Answer:")[-1].strip(), width=100)
    print("Question:", query)
    print("Answer:\n", final_answer)
else:
    print("No relevant context found.")